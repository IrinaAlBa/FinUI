{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2415f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b475ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 256 \n",
    "height = 192 \n",
    "channels = 3\n",
    "\n",
    "fin_dir = '../datasets/FinUI/images'\n",
    "fin_img = os.listdir(fin_dir)\n",
    "fin_img = [fin_dir + '/' + fi for fi in fin_img if fi.split('.')[1]=='png']\n",
    "\n",
    "X = []\n",
    "  \n",
    "for image in fin_img:\n",
    "    X.append(cv2.resize(cv2.imread(image, cv2.IMREAD_COLOR), (width, height), interpolation=cv2.INTER_AREA))\n",
    "    \n",
    "X_fin = np.array(X)\n",
    "\n",
    "ex_s = pd.read_csv('../datasets/FinUI/100_avg_scores.csv')\n",
    "y_fin = ex_s.mean(axis=1).values\n",
    "# y_fin = ex_s['3_1'].values\n",
    "\n",
    "\n",
    "X_train = X_fin[:70, :]\n",
    "y_train = y_fin[:70]\n",
    "\n",
    "X_val = X_fin[70:, :]\n",
    "y_val = y_fin[70:]\n",
    "\n",
    "ntrain = len(X_train)\n",
    "nval = len(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77e1af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import img_to_array, load_img\n",
    "import h5py\n",
    "\n",
    "def traverse_datasets(hdf_file):\n",
    "\n",
    "    def h5py_dataset_iterator(g, prefix=''):\n",
    "        for key in g.keys():\n",
    "            item = g[key]\n",
    "            path = f'{prefix}/{key}'\n",
    "            if isinstance(item, h5py.Dataset): # test for dataset\n",
    "                yield (path, item)\n",
    "            elif isinstance(item, h5py.Group): # test for group (go down)\n",
    "                yield from h5py_dataset_iterator(item, path)\n",
    "\n",
    "    with h5py.File(hdf_file, 'r') as f:\n",
    "        for path, _ in h5py_dataset_iterator(f):\n",
    "            yield path\n",
    "            \n",
    "weights = {}\n",
    "filename = 'flickr_style.h5'\n",
    "\n",
    "with h5py.File(filename, 'r') as f:\n",
    "    for dset in traverse_datasets(filename):\n",
    "        weights[dset] = f[dset][:]\n",
    "\n",
    "conv1_bias = weights['/conv1/conv1/bias:0']\n",
    "conv1_kernel = weights['/conv1/conv1/kernel:0']\n",
    "conv2_bias = weights['/conv2/conv2/bias:0']\n",
    "conv2_kernel = weights['/conv2/conv2/kernel:0']\n",
    "conv3_bias = weights['/conv3/conv3/bias:0']\n",
    "conv3_kernel = weights['/conv3/conv3/kernel:0']\n",
    "conv4_bias = weights['/conv4/conv4/bias:0']\n",
    "conv4_kernel = weights['/conv4/conv4/kernel:0']\n",
    "conv5_bias = weights['/conv5/conv5/bias:0']\n",
    "conv5_kernel = weights['/conv5/conv5/kernel:0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfc3303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer\n",
    "from keras import backend as K\n",
    "\n",
    "class LRN(Layer):\n",
    "    \n",
    "    def __init__(self, n=5, alpha=0.0001, beta=0.75, k=2, **kwargs):\n",
    "        self.n = n\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.k = k\n",
    "        super(LRN, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.shape = input_shape\n",
    "        super(LRN, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        if K.image_data_format == \"th\":\n",
    "            _, f, r, c = self.shape\n",
    "        else:\n",
    "            _, r, c, f = self.shape\n",
    "        half_n = self.n // 2\n",
    "        squared = K.square(x)\n",
    "        pooled = K.pool2d(squared, (half_n, half_n), strides=(1, 1),\n",
    "                         padding=\"same\", pool_mode=\"avg\")\n",
    "        if K.image_data_format == \"th\":\n",
    "            summed = K.sum(pooled, axis=1, keepdims=True)\n",
    "            averaged = (self.alpha / self.n) * K.repeat_elements(summed, f, axis=1)\n",
    "        else:\n",
    "            summed = K.sum(pooled, axis=3, keepdims=True)\n",
    "            averaged = (self.alpha / self.n) * K.repeat_elements(summed, f, axis=3)\n",
    "        denom = K.pow(self.k + averaged, self.beta)\n",
    "        return x / denom\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"n\": self.n,\n",
    "            \"alpha\": self.alpha,\n",
    "            \"beta\": self.beta,\n",
    "            \"k\": self.k\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f855a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 0.001 # weight decay\n",
    "\n",
    "input_shape = (192, 256, 3)\n",
    "\n",
    "im_data = layers.Input(shape=input_shape, dtype='float32', name='im_data')\n",
    "\n",
    "conv1 = layers.Conv2D(96, kernel_size=(11, 11), strides=(4, 4), name='conv1', \n",
    "                        activation='relu', input_shape=input_shape, \n",
    "                        kernel_regularizer=regularizers.l2(l))(im_data)\n",
    "\n",
    "pool1 = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(conv1)\n",
    "norm1 = LRN(name=\"norm1\")(pool1)\n",
    "drop1 = layers.Dropout(0.1)(norm1)\n",
    "\n",
    "layer1_1 = layers.Lambda(lambda x: x[:, :, :, :48])(drop1)\n",
    "layer1_2 = layers.Lambda(lambda x: x[:, :, :, 48:])(drop1)\n",
    "\n",
    "conv2_1 = layers.Conv2D(128, kernel_size=(5, 5), strides=(1, 1),\n",
    "                        activation='relu',\n",
    "                        padding='same', \n",
    "                        name='conv2_1', \n",
    "                        kernel_regularizer=regularizers.l2(l))(layer1_1)\n",
    "\n",
    "conv2_2 = layers.Conv2D(128, kernel_size=(5, 5), strides=(1, 1),\n",
    "                        activation='relu',\n",
    "                        padding='same', \n",
    "                        name='conv2_2',\n",
    "                        kernel_regularizer=regularizers.l2(l))(layer1_2)\n",
    "\n",
    "conv2 = layers.Concatenate(name='conv_2')([conv2_1, conv2_2])\n",
    "\n",
    "pool2 = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(conv2)\n",
    "norm2 = LRN(name=\"norm2\")(pool2)\n",
    "drop2 = layers.Dropout(0.1)(norm2)\n",
    "\n",
    "conv3 = layers.Conv2D(384, kernel_size=(3, 3), strides=(1, 1), activation='relu', \n",
    "                        name='conv3',\n",
    "                        padding='same',\n",
    "                        kernel_regularizer=regularizers.l2(l))(drop2)\n",
    "drop3 = layers.Dropout(0.1)(conv3)\n",
    "\n",
    "layer3_1 = layers.Lambda(lambda x: x[:, :, :, :192])(drop3)\n",
    "layer3_2 = layers.Lambda(lambda x: x[:, :, :, 192:])(drop3)\n",
    "\n",
    "conv4_1 = layers.Conv2D(192, kernel_size=(3, 3), strides=(1, 1),\n",
    "                        activation='relu', \n",
    "                        padding='same',\n",
    "                        name='conv4_1',\n",
    "                        kernel_regularizer=regularizers.l2(l))(layer3_1)\n",
    "\n",
    "conv4_2 = layers.Conv2D(192, kernel_size=(3, 3), strides=(1, 1),\n",
    "                        activation='relu', \n",
    "                        padding='same',\n",
    "                        name='conv4_2',\n",
    "                        kernel_regularizer=regularizers.l2(l))(layer3_2)\n",
    "\n",
    "conv4 = layers.Concatenate(name='conv_4')([conv4_1, conv4_2])\n",
    "\n",
    "layer4_1 = layers.Lambda(lambda x: x[:, :, :, :192])(conv4)\n",
    "layer4_2 = layers.Lambda(lambda x: x[:, :, :, 192:])(conv4)\n",
    "\n",
    "conv5_1 = layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1),\n",
    "                        activation='relu',\n",
    "                        padding='same', \n",
    "                        name='conv5_1',\n",
    "                        kernel_regularizer=regularizers.l2(l))(layer4_1)\n",
    "\n",
    "conv5_2 = layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1),\n",
    "                        activation='relu',\n",
    "                        padding='same', \n",
    "                        name='conv5_2',\n",
    "                        kernel_regularizer=regularizers.l2(l))(layer4_2)\n",
    "\n",
    "conv5 = layers.Concatenate(name='conv_5')([conv5_1, conv5_2])\n",
    "\n",
    "pool5 = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(conv5)\n",
    "\n",
    "flat = layers.Flatten()(pool5)\n",
    "fc6 = layers.Dense(1024, activation='relu', name='fc6',\n",
    "                        kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "                        bias_initializer='zeros',\n",
    "                        kernel_regularizer=regularizers.l2(l))(flat)\n",
    "drop6 = layers.Dropout(0.5)(fc6)\n",
    "\n",
    "fc7 = layers.Dense(512, activation='relu', name='fc7', \n",
    "                        kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "                        bias_initializer='zeros',\n",
    "                        kernel_regularizer=regularizers.l2(l))(drop6)\n",
    "drop7 = layers.Dropout(0.5)(fc7)\n",
    "\n",
    "fc8 = layers.Dense(1, name='fc8',\n",
    "                        kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "                        bias_initializer='zeros')(drop7)\n",
    "\n",
    "model = models.Model(inputs=im_data, outputs=fc8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8699649e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_layer('conv1').set_weights([conv1_kernel[:, :, :, :], conv1_bias[:]])\n",
    "model.get_layer('conv2_1').set_weights([conv2_kernel[:, :, :, :128], conv2_bias[:128]])\n",
    "model.get_layer('conv2_2').set_weights([conv2_kernel[:, :, :, 128:], conv2_bias[128:]])\n",
    "model.get_layer('conv3').set_weights([conv3_kernel[:, :, :, :], conv3_bias[:]])\n",
    "model.get_layer('conv4_1').set_weights([conv4_kernel[:, :, :, :192], conv4_bias[:192]])\n",
    "model.get_layer('conv4_2').set_weights([conv4_kernel[:, :, :, 192:], conv4_bias[192:]])\n",
    "model.get_layer('conv5_1').set_weights([conv5_kernel[:, :, :, :128], conv5_bias[:128]])\n",
    "model.get_layer('conv5_2').set_weights([conv5_kernel[:, :, :, 128:], conv5_bias[128:]])\n",
    "\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db82d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualkeras\n",
    "from PIL import ImageFont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde4cea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualkeras.layered_view(model,legend=True, draw_volume=True, font=ImageFont.truetype(\"arial.ttf\", 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a1deff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "\treturn K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "def euclidean_distance_loss(y_true, y_pred):\n",
    "    return 0.5 * K.mean(K.square(y_pred - y_true), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357f0f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255)                                  \n",
    "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)\n",
    "val_generator = val_datagen.flow(X_val, y_val, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dd8350",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 150\n",
    "decay = 0.001 \n",
    "base_lr = 0.01\n",
    "\n",
    "sgd = optimizers.SGD(learning_rate=base_lr, momentum=0.9, decay=decay, nesterov=True)\n",
    "model.compile(loss=euclidean_distance_loss , optimizer=sgd, metrics=[rmse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac4b41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,                          \n",
    "    steps_per_epoch = ntrain // batch_size,                           \n",
    "    epochs = epochs,                          \n",
    "    validation_data=val_generator,                         \n",
    "    validation_steps=nval // batch_size,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708c1985",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = history.history[\"rmse\"]\n",
    "val_rmse = history.history[\"val_rmse\"]\n",
    "\n",
    "epochs_x = range(1, len(rmse) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a634174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs_x, rmse, 'b', label='Training RMSE')\n",
    "plt.plot(epochs_x, val_rmse, 'r', label='Validation RMSE')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d7bdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def pearsonr_ci(x,y,alpha=0.05):\n",
    "    ''' calculate Pearson correlation along with the confidence interval using scipy and numpy\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : iterable object such as a list or np.array\n",
    "      Input for correlation calculation\n",
    "    alpha : float\n",
    "      Significance level. 0.05 by default\n",
    "    Returns\n",
    "    -------\n",
    "    r : float\n",
    "      Pearson's correlation coefficient\n",
    "    pval : float\n",
    "      The corresponding p value\n",
    "    lo, hi : float\n",
    "      The lower and upper bound of confidence intervals\n",
    "    '''\n",
    "    N = len(x)\n",
    "    r, p = stats.pearsonr(x,y)\n",
    "    r_z = np.arctanh(r)\n",
    "    se = 1/np.sqrt(N-3)\n",
    "    z = stats.norm.ppf(1-alpha/2)\n",
    "    lo_z, hi_z = r_z-z*se, r_z+z*se\n",
    "    lo, hi = np.tanh((lo_z, hi_z))\n",
    "    return r, p, lo, hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa8ab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "X_test = X_fin / 255.0\n",
    "y_test = y_fin\n",
    "for img in X_test:\n",
    "  img = img.reshape(1, 192, 256, 3)\n",
    "  pred = model.predict(img, verbose=0)\n",
    "  predictions.append(float(pred))\n",
    "\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c104ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.polynomial.polynomial import polyfit\n",
    "b, m = polyfit(y_test, predictions, 1)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(y_test, predictions, c='c')\n",
    "plt.plot(y_test, b + m * y_test, '-', c='b')\n",
    "plt.xlabel('User ratings', labelpad=10, fontsize=16)\n",
    "plt.ylabel('Predicted ratings', labelpad=10, fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8baf73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "corr, p, lo, hi = pearsonr_ci(y_test, predictions)\n",
    "print('Pearsons correlation: r=%.2f, p=%.2e, CI=[%.2f, %.2f]' % (corr, p, lo, hi))\n",
    "rmse_test = sqrt(mean_squared_error(y_test, predictions))\n",
    "print('RMSE: %.3f' % rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ee8f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr, p, lo, hi = pearsonr_ci(y_val, predictions)\n",
    "print('Pearsons correlation: r=%.2f, p=%.2e, CI=[%.2f, %.2f]' % (corr, p, lo, hi))\n",
    "rmse_test = sqrt(mean_squared_error(y_val, predictions))\n",
    "print('RMSE: %.3f' % rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad7c02c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calista",
   "language": "python",
   "name": "calista"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
